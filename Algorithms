I. Linear Algorithms

    i. Simple Linear Regression: For predicting numerical values when there is only a single input.
    ii. Multivariate Linear Regression: For predicting numerical values with more than one input (trained using StochasticGradient Descent).
    iii. Logistic Regression: For predicting a class value on 2 class problems (trained using Stochastic Gradient Descent).
    iv. Perceptron: The simplest type of neural network for classification problems (trained using StochasticGradient Descent).

II. Nonlinear Algorithms

    i. Classification and Regression Trees: Decision trees, in this case applied to classification problems.
    ii. Naive Bayes: The very simple application of Bayesâ€™ Theorem to classification problems.
    iii. k-Nearest Neighbors: For predicting numerical or categorical outcomes directly from training data.
    iv. Learning Vector Quantization: A type of neural network that is more efficient than k-Nearest Neighbors.
    v. Backpropagation: The most widely used type of artificial neural network that underlies the broader field of deep learning.

III. Ensemble Algorithms

    i. Bootstrap Aggregation: Also known as bagging that involves an ensemble of decision trees.
    ii. Random Forest: An extension of bagging that results in faster training and better performance.
    iii. Stacked Aggregation: An ensemble method also known as stacking or blending that learns how to best combine the perdictions from multiple models.
